{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jwZjfYyhshml"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time,datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.debug(\"This is a debug message\");\n",
        "logging.info(\"This is an info message\");\n",
        "logging.warning(\"This is a warning message\");\n",
        "logging.error(\"This is an error message\");\n",
        "logging.fatal(\"This is a critical error\");\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk98H3tftReZ",
        "outputId": "403480a9-fbf9-4225-fff7-800671a0612a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:This is a warning message\n",
            "ERROR:root:This is an error message\n",
            "CRITICAL:root:This is a critical error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logger=logging.getLogger(\"log4j\")\n",
        "logger.setLevel(logging.DEBUG)"
      ],
      "metadata": {
        "id": "LvASvxIPuIkU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See I used log4j logger and I am \\setting up level as DEBUG we can alo also levels as info,error See if you use level as  debug gives then it includes info and error messages ,info gives only info ,error gives only error so better use debug level and formatter will help to store the file name as in the particular format"
      ],
      "metadata": {
        "id": "GSeD7w9ByUFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formatter = logging.Formatter(\n",
        "    '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s: %(message)s',\n",
        "    datefmt='%d/%m/%Y %I:%M:%S %p'\n",
        ")  #first formatter"
      ],
      "metadata": {
        "id": "JZGJ9Ekkv8zy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatter = logging.Formatter(\n",
        "    '%(levelname)s (%(asctime)s): %(message)s (Line: %(lineno)d [%(filename)s])',\n",
        "    datefmt='%d/%m/%Y %I:%M:%S %p'\n",
        ") #second formatter"
      ],
      "metadata": {
        "id": "vNLpH2ma_M8F"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "decide how your log file should look like with formatter and if data is from multiple timezone use python module to handle that"
      ],
      "metadata": {
        "id": "ybO5ulb9Avhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create file handler which logs even debug messages\n",
        "log_filename = f\"/content/sample_data/{datetime.datetime.now().strftime('%Y-%m-%d-%H')}.log\"\n",
        "\n",
        "logFileHandler = logging.FileHandler(log_filename, mode='w')  # 'a' will append\n",
        "logFileHandler.setFormatter(formatter)\n",
        "logger.addHandler(logFileHandler)\n"
      ],
      "metadata": {
        "id": "mdcifCmOlD25"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See what does filehandler does is it stores the file in the particular folder and tell which format and add that logfile to the log4j  logger"
      ],
      "metadata": {
        "id": "HqshblZFycZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('Dataframe').getOrCreate() #starting spark session"
      ],
      "metadata": {
        "id": "aXmZH3-U6bS2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.csv('/content/test1.csv',header=True,inferSchema=True) #reading csv file\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtEG91fh7dNV",
        "outputId": "130772b3-6ae3-4ae2-8603-aaec58cf8af5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "|    Sunny| 29|         4| 20000|\n",
            "|     Paul| 24|         3| 20000|\n",
            "|   Harsha| 21|         1| 15000|\n",
            "|  Shubham| 23|         2| 18000|\n",
            "+---------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = '{} Share data source file read started................'.format('MSFT')\n",
        "logger.info(message)\n",
        "\n",
        "df = spark.read.csv('/content/test1.csv', header='true')\n",
        "\n",
        "count = df.count()\n",
        "print(count)\n",
        "\n",
        "message = '{} Total number of {} records present'.format('MSFT', count)\n",
        "logger.info(message)\n",
        "\n",
        "try:\n",
        "    df1 = df.withColumn('testColumn', str(df.column_not_present)) # I am raising error by calling column function which is not there\n",
        "    display(df1)\n",
        "\n",
        "except Exception as e:\n",
        "    print('Logging Error {}'.format(e))\n",
        "    errorMessage = 'Logging Error defined : {}'.format(e)\n",
        "    logger.error(errorMessage)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNRsz0P58GM3",
        "outputId": "9ed6e024-dacb-4f7b-c030-7beb110432a0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:log4j:MSFT Share data source file read started................\n",
            "INFO:log4j:MSFT Total number of 6 records present\n",
            "ERROR:log4j:Logging Error defined : 'DataFrame' object has no attribute 'column_not_present'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "Logging Error 'DataFrame' object has no attribute 'column_not_present'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.shutdown()"
      ],
      "metadata": {
        "id": "Q8gKzoUuwCwG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "de=spark.read.text('/content/sample_data/2025-04-01-20.log') #create by first formatter\n",
        "de.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXxGydHGwmEc",
        "outputId": "5150b698-df81-490a-8c06-91ab9ff81e31"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|value                                                                                                                                                        |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|INFO (01/04/2025 08:46:04 PM): MSFT Share data source file read started................ (Line: 2 [<ipython-input-39-b72795450567>])                          |\n",
            "|INFO (01/04/2025 08:46:05 PM): MSFT Total number of 6 records present (Line: 10 [<ipython-input-39-b72795450567>])                                           |\n",
            "|ERROR (01/04/2025 08:46:05 PM): Logging Error defined : 'DataFrame' object has no attribute 'column_not_present' (Line: 19 [<ipython-input-39-b72795450567>])|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "de=spark.read.text('/content/sample_data/abc.log') #created by second formatter\n",
        "de.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxkfvpviBEyl",
        "outputId": "7e789801-4ccc-4762-c26d-59e67b30f113"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|value                                                                                                                                                        |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|INFO (01/04/2025 08:21:25 PM): MSFT Share data source file read started................ (Line: 2 [<ipython-input-27-705433e7f824>])                          |\n",
            "|INFO (01/04/2025 08:21:26 PM): MSFT Total number of 6 records present (Line: 10 [<ipython-input-27-705433e7f824>])                                           |\n",
            "|ERROR (01/04/2025 08:21:26 PM): Logging Error defined : 'DataFrame' object has no attribute 'column_not_present' (Line: 19 [<ipython-input-27-705433e7f824>])|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}